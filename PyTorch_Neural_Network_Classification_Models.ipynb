{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead2fd22",
   "metadata": {},
   "source": [
    "# PyTorch Neural Network Classification Models\n",
    "\n",
    "This notebook contains two separate PyTorch neural network classification models:\n",
    "1. **Celebrity Dataset (CelebA)**\n",
    "2. **Handwritten Digit Images (MNIST)**\n",
    "\n",
    "Both models will be trained, validated, and tested using their respective datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba0746dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b95611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs available: 1\n",
      "GPU Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# Check the number of GPUs available\n",
    "gpu_count = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {gpu_count}\")\n",
    "\n",
    "# Get the name of the GPU\n",
    "if cuda_available:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU Name: {gpu_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84894c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Preparing the Datasets\n",
    "\n",
    "# Transformation for both datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize for CelebA compatibility\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# CelebA Dataset\n",
    "celeba_dataset = torchvision.datasets.CelebA(root='./data', split='train', download=True, transform=transform)\n",
    "celeba_loader = DataLoader(celeba_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# MNIST Dataset\n",
    "mnist_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_loader = DataLoader(mnist_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# For testing and validation\n",
    "celeba_test_dataset = torchvision.datasets.CelebA(root='./data', split='test', download=True, transform=transform)\n",
    "celeba_test_loader = DataLoader(celeba_test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "mnist_test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "mnist_test_loader = DataLoader(mnist_test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cd6be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Neural Network for CelebA (CNN)\n",
    "\n",
    "# Modify the last layer to output 40 values instead of 1\n",
    "class CelebACNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CelebACNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 40)  # Output 40 labels\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "celeba_model = CelebACNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(celeba_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c0c5d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/5087], Loss: 0.4287\n",
      "Epoch [1/5], Step [200/5087], Loss: 0.3740\n",
      "Epoch [1/5], Step [300/5087], Loss: 0.3396\n",
      "Epoch [1/5], Step [400/5087], Loss: 0.3164\n",
      "Epoch [1/5], Step [500/5087], Loss: 0.3061\n",
      "Epoch [1/5], Step [600/5087], Loss: 0.2948\n",
      "Epoch [1/5], Step [700/5087], Loss: 0.2864\n",
      "Epoch [1/5], Step [800/5087], Loss: 0.2837\n",
      "Epoch [1/5], Step [900/5087], Loss: 0.2774\n",
      "Epoch [1/5], Step [1000/5087], Loss: 0.2766\n",
      "Epoch [1/5], Step [1100/5087], Loss: 0.2713\n",
      "Epoch [1/5], Step [1200/5087], Loss: 0.2678\n",
      "Epoch [1/5], Step [1300/5087], Loss: 0.2672\n",
      "Epoch [1/5], Step [1400/5087], Loss: 0.2656\n",
      "Epoch [1/5], Step [1500/5087], Loss: 0.2620\n",
      "Epoch [1/5], Step [1600/5087], Loss: 0.2626\n",
      "Epoch [1/5], Step [1700/5087], Loss: 0.2596\n",
      "Epoch [1/5], Step [1800/5087], Loss: 0.2571\n",
      "Epoch [1/5], Step [1900/5087], Loss: 0.2569\n",
      "Epoch [1/5], Step [2000/5087], Loss: 0.2520\n",
      "Epoch [1/5], Step [2100/5087], Loss: 0.2573\n",
      "Epoch [1/5], Step [2200/5087], Loss: 0.2547\n",
      "Epoch [1/5], Step [2300/5087], Loss: 0.2502\n",
      "Epoch [1/5], Step [2400/5087], Loss: 0.2528\n",
      "Epoch [1/5], Step [2500/5087], Loss: 0.2489\n",
      "Epoch [1/5], Step [2600/5087], Loss: 0.2476\n",
      "Epoch [1/5], Step [2700/5087], Loss: 0.2509\n",
      "Epoch [1/5], Step [2800/5087], Loss: 0.2481\n",
      "Epoch [1/5], Step [2900/5087], Loss: 0.2469\n",
      "Epoch [1/5], Step [3000/5087], Loss: 0.2453\n",
      "Epoch [1/5], Step [3100/5087], Loss: 0.2447\n",
      "Epoch [1/5], Step [3200/5087], Loss: 0.2418\n",
      "Epoch [1/5], Step [3300/5087], Loss: 0.2430\n",
      "Epoch [1/5], Step [3400/5087], Loss: 0.2434\n",
      "Epoch [1/5], Step [3500/5087], Loss: 0.2446\n",
      "Epoch [1/5], Step [3600/5087], Loss: 0.2431\n",
      "Epoch [1/5], Step [3700/5087], Loss: 0.2433\n",
      "Epoch [1/5], Step [3800/5087], Loss: 0.2401\n",
      "Epoch [1/5], Step [3900/5087], Loss: 0.2392\n",
      "Epoch [1/5], Step [4000/5087], Loss: 0.2392\n",
      "Epoch [1/5], Step [4100/5087], Loss: 0.2411\n",
      "Epoch [1/5], Step [4200/5087], Loss: 0.2413\n",
      "Epoch [1/5], Step [4300/5087], Loss: 0.2394\n",
      "Epoch [1/5], Step [4400/5087], Loss: 0.2348\n",
      "Epoch [1/5], Step [4500/5087], Loss: 0.2383\n",
      "Epoch [1/5], Step [4600/5087], Loss: 0.2371\n",
      "Epoch [1/5], Step [4700/5087], Loss: 0.2372\n",
      "Epoch [1/5], Step [4800/5087], Loss: 0.2370\n",
      "Epoch [1/5], Step [4900/5087], Loss: 0.2325\n",
      "Epoch [1/5], Step [5000/5087], Loss: 0.2365\n",
      "Epoch [2/5], Step [100/5087], Loss: 0.2304\n",
      "Epoch [2/5], Step [200/5087], Loss: 0.2299\n",
      "Epoch [2/5], Step [300/5087], Loss: 0.2298\n",
      "Epoch [2/5], Step [400/5087], Loss: 0.2319\n",
      "Epoch [2/5], Step [500/5087], Loss: 0.2281\n",
      "Epoch [2/5], Step [600/5087], Loss: 0.2283\n",
      "Epoch [2/5], Step [700/5087], Loss: 0.2299\n",
      "Epoch [2/5], Step [800/5087], Loss: 0.2303\n",
      "Epoch [2/5], Step [900/5087], Loss: 0.2300\n",
      "Epoch [2/5], Step [1000/5087], Loss: 0.2302\n",
      "Epoch [2/5], Step [1100/5087], Loss: 0.2295\n",
      "Epoch [2/5], Step [1200/5087], Loss: 0.2278\n",
      "Epoch [2/5], Step [1300/5087], Loss: 0.2284\n",
      "Epoch [2/5], Step [1400/5087], Loss: 0.2302\n",
      "Epoch [2/5], Step [1500/5087], Loss: 0.2270\n",
      "Epoch [2/5], Step [1600/5087], Loss: 0.2267\n",
      "Epoch [2/5], Step [1700/5087], Loss: 0.2283\n",
      "Epoch [2/5], Step [1800/5087], Loss: 0.2257\n",
      "Epoch [2/5], Step [1900/5087], Loss: 0.2265\n",
      "Epoch [2/5], Step [2000/5087], Loss: 0.2265\n",
      "Epoch [2/5], Step [2100/5087], Loss: 0.2286\n",
      "Epoch [2/5], Step [2200/5087], Loss: 0.2299\n",
      "Epoch [2/5], Step [2300/5087], Loss: 0.2275\n",
      "Epoch [2/5], Step [2400/5087], Loss: 0.2269\n",
      "Epoch [2/5], Step [2500/5087], Loss: 0.2259\n",
      "Epoch [2/5], Step [2600/5087], Loss: 0.2260\n",
      "Epoch [2/5], Step [2700/5087], Loss: 0.2287\n",
      "Epoch [2/5], Step [2800/5087], Loss: 0.2243\n",
      "Epoch [2/5], Step [2900/5087], Loss: 0.2275\n",
      "Epoch [2/5], Step [3000/5087], Loss: 0.2244\n",
      "Epoch [2/5], Step [3100/5087], Loss: 0.2267\n",
      "Epoch [2/5], Step [3200/5087], Loss: 0.2275\n",
      "Epoch [2/5], Step [3300/5087], Loss: 0.2243\n",
      "Epoch [2/5], Step [3400/5087], Loss: 0.2266\n",
      "Epoch [2/5], Step [3500/5087], Loss: 0.2275\n",
      "Epoch [2/5], Step [3600/5087], Loss: 0.2284\n",
      "Epoch [2/5], Step [3700/5087], Loss: 0.2247\n",
      "Epoch [2/5], Step [3800/5087], Loss: 0.2230\n",
      "Epoch [2/5], Step [3900/5087], Loss: 0.2282\n",
      "Epoch [2/5], Step [4000/5087], Loss: 0.2242\n",
      "Epoch [2/5], Step [4100/5087], Loss: 0.2249\n",
      "Epoch [2/5], Step [4200/5087], Loss: 0.2259\n",
      "Epoch [2/5], Step [4300/5087], Loss: 0.2248\n",
      "Epoch [2/5], Step [4400/5087], Loss: 0.2266\n",
      "Epoch [2/5], Step [4500/5087], Loss: 0.2246\n",
      "Epoch [2/5], Step [4600/5087], Loss: 0.2246\n",
      "Epoch [2/5], Step [4700/5087], Loss: 0.2254\n",
      "Epoch [2/5], Step [4800/5087], Loss: 0.2217\n",
      "Epoch [2/5], Step [4900/5087], Loss: 0.2247\n",
      "Epoch [2/5], Step [5000/5087], Loss: 0.2214\n",
      "Epoch [3/5], Step [100/5087], Loss: 0.2147\n",
      "Epoch [3/5], Step [200/5087], Loss: 0.2150\n",
      "Epoch [3/5], Step [300/5087], Loss: 0.2154\n",
      "Epoch [3/5], Step [400/5087], Loss: 0.2165\n",
      "Epoch [3/5], Step [500/5087], Loss: 0.2180\n",
      "Epoch [3/5], Step [600/5087], Loss: 0.2151\n",
      "Epoch [3/5], Step [700/5087], Loss: 0.2165\n",
      "Epoch [3/5], Step [800/5087], Loss: 0.2191\n",
      "Epoch [3/5], Step [900/5087], Loss: 0.2169\n",
      "Epoch [3/5], Step [1000/5087], Loss: 0.2164\n",
      "Epoch [3/5], Step [1100/5087], Loss: 0.2170\n",
      "Epoch [3/5], Step [1200/5087], Loss: 0.2148\n",
      "Epoch [3/5], Step [1300/5087], Loss: 0.2159\n",
      "Epoch [3/5], Step [1400/5087], Loss: 0.2161\n",
      "Epoch [3/5], Step [1500/5087], Loss: 0.2171\n",
      "Epoch [3/5], Step [1600/5087], Loss: 0.2200\n",
      "Epoch [3/5], Step [1700/5087], Loss: 0.2164\n",
      "Epoch [3/5], Step [1800/5087], Loss: 0.2182\n",
      "Epoch [3/5], Step [1900/5087], Loss: 0.2183\n",
      "Epoch [3/5], Step [2000/5087], Loss: 0.2147\n",
      "Epoch [3/5], Step [2100/5087], Loss: 0.2188\n",
      "Epoch [3/5], Step [2200/5087], Loss: 0.2164\n",
      "Epoch [3/5], Step [2300/5087], Loss: 0.2149\n",
      "Epoch [3/5], Step [2400/5087], Loss: 0.2144\n",
      "Epoch [3/5], Step [2500/5087], Loss: 0.2172\n",
      "Epoch [3/5], Step [2600/5087], Loss: 0.2163\n",
      "Epoch [3/5], Step [2700/5087], Loss: 0.2187\n",
      "Epoch [3/5], Step [2800/5087], Loss: 0.2150\n",
      "Epoch [3/5], Step [2900/5087], Loss: 0.2175\n",
      "Epoch [3/5], Step [3000/5087], Loss: 0.2148\n",
      "Epoch [3/5], Step [3100/5087], Loss: 0.2167\n",
      "Epoch [3/5], Step [3200/5087], Loss: 0.2150\n",
      "Epoch [3/5], Step [3300/5087], Loss: 0.2170\n",
      "Epoch [3/5], Step [3400/5087], Loss: 0.2171\n",
      "Epoch [3/5], Step [3500/5087], Loss: 0.2162\n",
      "Epoch [3/5], Step [3600/5087], Loss: 0.2154\n",
      "Epoch [3/5], Step [3700/5087], Loss: 0.2167\n",
      "Epoch [3/5], Step [3800/5087], Loss: 0.2189\n",
      "Epoch [3/5], Step [3900/5087], Loss: 0.2147\n",
      "Epoch [3/5], Step [4000/5087], Loss: 0.2160\n",
      "Epoch [3/5], Step [4100/5087], Loss: 0.2180\n",
      "Epoch [3/5], Step [4200/5087], Loss: 0.2159\n",
      "Epoch [3/5], Step [4300/5087], Loss: 0.2189\n",
      "Epoch [3/5], Step [4400/5087], Loss: 0.2151\n",
      "Epoch [3/5], Step [4500/5087], Loss: 0.2163\n",
      "Epoch [3/5], Step [4600/5087], Loss: 0.2163\n",
      "Epoch [3/5], Step [4700/5087], Loss: 0.2173\n",
      "Epoch [3/5], Step [4800/5087], Loss: 0.2160\n",
      "Epoch [3/5], Step [4900/5087], Loss: 0.2171\n",
      "Epoch [3/5], Step [5000/5087], Loss: 0.2176\n",
      "Epoch [4/5], Step [100/5087], Loss: 0.2057\n",
      "Epoch [4/5], Step [200/5087], Loss: 0.2086\n",
      "Epoch [4/5], Step [300/5087], Loss: 0.2071\n",
      "Epoch [4/5], Step [400/5087], Loss: 0.2084\n",
      "Epoch [4/5], Step [500/5087], Loss: 0.2088\n",
      "Epoch [4/5], Step [600/5087], Loss: 0.2060\n",
      "Epoch [4/5], Step [700/5087], Loss: 0.2088\n",
      "Epoch [4/5], Step [800/5087], Loss: 0.2078\n",
      "Epoch [4/5], Step [900/5087], Loss: 0.2062\n",
      "Epoch [4/5], Step [1000/5087], Loss: 0.2079\n",
      "Epoch [4/5], Step [1100/5087], Loss: 0.2076\n",
      "Epoch [4/5], Step [1200/5087], Loss: 0.2075\n",
      "Epoch [4/5], Step [1300/5087], Loss: 0.2085\n",
      "Epoch [4/5], Step [1400/5087], Loss: 0.2079\n",
      "Epoch [4/5], Step [1500/5087], Loss: 0.2101\n",
      "Epoch [4/5], Step [1600/5087], Loss: 0.2087\n",
      "Epoch [4/5], Step [1700/5087], Loss: 0.2103\n",
      "Epoch [4/5], Step [1800/5087], Loss: 0.2073\n",
      "Epoch [4/5], Step [1900/5087], Loss: 0.2098\n",
      "Epoch [4/5], Step [2000/5087], Loss: 0.2118\n",
      "Epoch [4/5], Step [2100/5087], Loss: 0.2101\n",
      "Epoch [4/5], Step [2200/5087], Loss: 0.2113\n",
      "Epoch [4/5], Step [2300/5087], Loss: 0.2073\n",
      "Epoch [4/5], Step [2400/5087], Loss: 0.2119\n",
      "Epoch [4/5], Step [2500/5087], Loss: 0.2093\n",
      "Epoch [4/5], Step [2600/5087], Loss: 0.2105\n",
      "Epoch [4/5], Step [2700/5087], Loss: 0.2066\n",
      "Epoch [4/5], Step [2800/5087], Loss: 0.2085\n",
      "Epoch [4/5], Step [2900/5087], Loss: 0.2125\n",
      "Epoch [4/5], Step [3000/5087], Loss: 0.2099\n",
      "Epoch [4/5], Step [3100/5087], Loss: 0.2092\n",
      "Epoch [4/5], Step [3200/5087], Loss: 0.2107\n",
      "Epoch [4/5], Step [3300/5087], Loss: 0.2102\n",
      "Epoch [4/5], Step [3400/5087], Loss: 0.2128\n",
      "Epoch [4/5], Step [3500/5087], Loss: 0.2121\n",
      "Epoch [4/5], Step [3600/5087], Loss: 0.2112\n",
      "Epoch [4/5], Step [3700/5087], Loss: 0.2137\n",
      "Epoch [4/5], Step [3800/5087], Loss: 0.2100\n",
      "Epoch [4/5], Step [3900/5087], Loss: 0.2111\n",
      "Epoch [4/5], Step [4000/5087], Loss: 0.2083\n",
      "Epoch [4/5], Step [4100/5087], Loss: 0.2143\n",
      "Epoch [4/5], Step [4200/5087], Loss: 0.2090\n",
      "Epoch [4/5], Step [4300/5087], Loss: 0.2113\n",
      "Epoch [4/5], Step [4400/5087], Loss: 0.2104\n",
      "Epoch [4/5], Step [4500/5087], Loss: 0.2114\n",
      "Epoch [4/5], Step [4600/5087], Loss: 0.2113\n",
      "Epoch [4/5], Step [4700/5087], Loss: 0.2131\n",
      "Epoch [4/5], Step [4800/5087], Loss: 0.2122\n",
      "Epoch [4/5], Step [4900/5087], Loss: 0.2102\n",
      "Epoch [4/5], Step [5000/5087], Loss: 0.2091\n",
      "Epoch [5/5], Step [100/5087], Loss: 0.2008\n",
      "Epoch [5/5], Step [200/5087], Loss: 0.2028\n",
      "Epoch [5/5], Step [300/5087], Loss: 0.1987\n",
      "Epoch [5/5], Step [400/5087], Loss: 0.2018\n",
      "Epoch [5/5], Step [500/5087], Loss: 0.1971\n",
      "Epoch [5/5], Step [600/5087], Loss: 0.2022\n",
      "Epoch [5/5], Step [700/5087], Loss: 0.2015\n",
      "Epoch [5/5], Step [800/5087], Loss: 0.2004\n",
      "Epoch [5/5], Step [900/5087], Loss: 0.2021\n",
      "Epoch [5/5], Step [1000/5087], Loss: 0.2008\n",
      "Epoch [5/5], Step [1100/5087], Loss: 0.2009\n",
      "Epoch [5/5], Step [1200/5087], Loss: 0.2026\n",
      "Epoch [5/5], Step [1300/5087], Loss: 0.2023\n",
      "Epoch [5/5], Step [1400/5087], Loss: 0.2015\n",
      "Epoch [5/5], Step [1500/5087], Loss: 0.2017\n",
      "Epoch [5/5], Step [1600/5087], Loss: 0.2045\n",
      "Epoch [5/5], Step [1700/5087], Loss: 0.2049\n",
      "Epoch [5/5], Step [1800/5087], Loss: 0.2045\n",
      "Epoch [5/5], Step [1900/5087], Loss: 0.2051\n",
      "Epoch [5/5], Step [2000/5087], Loss: 0.2038\n",
      "Epoch [5/5], Step [2100/5087], Loss: 0.2043\n",
      "Epoch [5/5], Step [2200/5087], Loss: 0.2047\n",
      "Epoch [5/5], Step [2300/5087], Loss: 0.2072\n",
      "Epoch [5/5], Step [2400/5087], Loss: 0.2043\n",
      "Epoch [5/5], Step [2500/5087], Loss: 0.2042\n",
      "Epoch [5/5], Step [2600/5087], Loss: 0.2038\n",
      "Epoch [5/5], Step [2700/5087], Loss: 0.2031\n",
      "Epoch [5/5], Step [2800/5087], Loss: 0.2037\n",
      "Epoch [5/5], Step [2900/5087], Loss: 0.2060\n",
      "Epoch [5/5], Step [3000/5087], Loss: 0.2069\n",
      "Epoch [5/5], Step [3100/5087], Loss: 0.2037\n",
      "Epoch [5/5], Step [3200/5087], Loss: 0.2026\n",
      "Epoch [5/5], Step [3300/5087], Loss: 0.2041\n",
      "Epoch [5/5], Step [3400/5087], Loss: 0.2040\n",
      "Epoch [5/5], Step [3500/5087], Loss: 0.2077\n",
      "Epoch [5/5], Step [3600/5087], Loss: 0.2059\n",
      "Epoch [5/5], Step [3700/5087], Loss: 0.2047\n",
      "Epoch [5/5], Step [3800/5087], Loss: 0.2040\n",
      "Epoch [5/5], Step [3900/5087], Loss: 0.2072\n",
      "Epoch [5/5], Step [4000/5087], Loss: 0.2064\n",
      "Epoch [5/5], Step [4100/5087], Loss: 0.2073\n",
      "Epoch [5/5], Step [4200/5087], Loss: 0.2060\n",
      "Epoch [5/5], Step [4300/5087], Loss: 0.2051\n",
      "Epoch [5/5], Step [4400/5087], Loss: 0.2060\n",
      "Epoch [5/5], Step [4500/5087], Loss: 0.2055\n",
      "Epoch [5/5], Step [4600/5087], Loss: 0.2060\n",
      "Epoch [5/5], Step [4700/5087], Loss: 0.2052\n",
      "Epoch [5/5], Step [4800/5087], Loss: 0.2096\n",
      "Epoch [5/5], Step [4900/5087], Loss: 0.2064\n",
      "Epoch [5/5], Step [5000/5087], Loss: 0.2055\n",
      "Finished Training CelebA Model\n"
     ]
    }
   ],
   "source": [
    "# Training the CelebA Model\n",
    "\n",
    "epochs = 5\n",
    "celeba_model = CelebACNN().cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(celeba_model.parameters(), lr=0.001)\n",
    "celeba_model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(celeba_loader, 0):\n",
    "        inputs, labels = data\n",
    "        labels = labels.float().cuda()  # Ensure labels are on GPU and correctly shaped\n",
    "        optimizer.zero_grad()\n",
    "        outputs = celeba_model(inputs.cuda())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(celeba_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training CelebA Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4d8849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Neural Network for MNIST (CNN)\n",
    "\n",
    "class MNISTCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "mnist_model = MNISTCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mnist_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bf0358d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/1875], Loss: 1.1105\n",
      "Epoch [1/5], Step [200/1875], Loss: 0.3680\n",
      "Epoch [1/5], Step [300/1875], Loss: 0.2389\n",
      "Epoch [1/5], Step [400/1875], Loss: 0.1657\n",
      "Epoch [1/5], Step [500/1875], Loss: 0.1576\n",
      "Epoch [1/5], Step [600/1875], Loss: 0.1308\n",
      "Epoch [1/5], Step [700/1875], Loss: 0.1375\n",
      "Epoch [1/5], Step [800/1875], Loss: 0.1038\n",
      "Epoch [1/5], Step [900/1875], Loss: 0.0906\n",
      "Epoch [1/5], Step [1000/1875], Loss: 0.0938\n",
      "Epoch [1/5], Step [1100/1875], Loss: 0.0942\n",
      "Epoch [1/5], Step [1200/1875], Loss: 0.0788\n",
      "Epoch [1/5], Step [1300/1875], Loss: 0.0907\n",
      "Epoch [1/5], Step [1400/1875], Loss: 0.0849\n",
      "Epoch [1/5], Step [1500/1875], Loss: 0.0779\n",
      "Epoch [1/5], Step [1600/1875], Loss: 0.0902\n",
      "Epoch [1/5], Step [1700/1875], Loss: 0.0760\n",
      "Epoch [1/5], Step [1800/1875], Loss: 0.0714\n",
      "Epoch [2/5], Step [100/1875], Loss: 0.0639\n",
      "Epoch [2/5], Step [200/1875], Loss: 0.0568\n",
      "Epoch [2/5], Step [300/1875], Loss: 0.0600\n",
      "Epoch [2/5], Step [400/1875], Loss: 0.0534\n",
      "Epoch [2/5], Step [500/1875], Loss: 0.0484\n",
      "Epoch [2/5], Step [600/1875], Loss: 0.0672\n",
      "Epoch [2/5], Step [700/1875], Loss: 0.0519\n",
      "Epoch [2/5], Step [800/1875], Loss: 0.0542\n",
      "Epoch [2/5], Step [900/1875], Loss: 0.0576\n",
      "Epoch [2/5], Step [1000/1875], Loss: 0.0600\n",
      "Epoch [2/5], Step [1100/1875], Loss: 0.0456\n",
      "Epoch [2/5], Step [1200/1875], Loss: 0.0434\n",
      "Epoch [2/5], Step [1300/1875], Loss: 0.0414\n",
      "Epoch [2/5], Step [1400/1875], Loss: 0.0515\n",
      "Epoch [2/5], Step [1500/1875], Loss: 0.0486\n",
      "Epoch [2/5], Step [1600/1875], Loss: 0.0461\n",
      "Epoch [2/5], Step [1700/1875], Loss: 0.0455\n",
      "Epoch [2/5], Step [1800/1875], Loss: 0.0451\n",
      "Epoch [3/5], Step [100/1875], Loss: 0.0320\n",
      "Epoch [3/5], Step [200/1875], Loss: 0.0252\n",
      "Epoch [3/5], Step [300/1875], Loss: 0.0457\n",
      "Epoch [3/5], Step [400/1875], Loss: 0.0369\n",
      "Epoch [3/5], Step [500/1875], Loss: 0.0412\n",
      "Epoch [3/5], Step [600/1875], Loss: 0.0283\n",
      "Epoch [3/5], Step [700/1875], Loss: 0.0381\n",
      "Epoch [3/5], Step [800/1875], Loss: 0.0361\n",
      "Epoch [3/5], Step [900/1875], Loss: 0.0361\n",
      "Epoch [3/5], Step [1000/1875], Loss: 0.0326\n",
      "Epoch [3/5], Step [1100/1875], Loss: 0.0348\n",
      "Epoch [3/5], Step [1200/1875], Loss: 0.0416\n",
      "Epoch [3/5], Step [1300/1875], Loss: 0.0363\n",
      "Epoch [3/5], Step [1400/1875], Loss: 0.0400\n",
      "Epoch [3/5], Step [1500/1875], Loss: 0.0405\n",
      "Epoch [3/5], Step [1600/1875], Loss: 0.0306\n",
      "Epoch [3/5], Step [1700/1875], Loss: 0.0424\n",
      "Epoch [3/5], Step [1800/1875], Loss: 0.0353\n",
      "Epoch [4/5], Step [100/1875], Loss: 0.0232\n",
      "Epoch [4/5], Step [200/1875], Loss: 0.0230\n",
      "Epoch [4/5], Step [300/1875], Loss: 0.0385\n",
      "Epoch [4/5], Step [400/1875], Loss: 0.0177\n",
      "Epoch [4/5], Step [500/1875], Loss: 0.0257\n",
      "Epoch [4/5], Step [600/1875], Loss: 0.0285\n",
      "Epoch [4/5], Step [700/1875], Loss: 0.0388\n",
      "Epoch [4/5], Step [800/1875], Loss: 0.0238\n",
      "Epoch [4/5], Step [900/1875], Loss: 0.0231\n",
      "Epoch [4/5], Step [1000/1875], Loss: 0.0238\n",
      "Epoch [4/5], Step [1100/1875], Loss: 0.0170\n",
      "Epoch [4/5], Step [1200/1875], Loss: 0.0185\n",
      "Epoch [4/5], Step [1300/1875], Loss: 0.0336\n",
      "Epoch [4/5], Step [1400/1875], Loss: 0.0289\n",
      "Epoch [4/5], Step [1500/1875], Loss: 0.0245\n",
      "Epoch [4/5], Step [1600/1875], Loss: 0.0332\n",
      "Epoch [4/5], Step [1700/1875], Loss: 0.0353\n",
      "Epoch [4/5], Step [1800/1875], Loss: 0.0244\n",
      "Epoch [5/5], Step [100/1875], Loss: 0.0212\n",
      "Epoch [5/5], Step [200/1875], Loss: 0.0186\n",
      "Epoch [5/5], Step [300/1875], Loss: 0.0133\n",
      "Epoch [5/5], Step [400/1875], Loss: 0.0122\n",
      "Epoch [5/5], Step [500/1875], Loss: 0.0168\n",
      "Epoch [5/5], Step [600/1875], Loss: 0.0225\n",
      "Epoch [5/5], Step [700/1875], Loss: 0.0185\n",
      "Epoch [5/5], Step [800/1875], Loss: 0.0213\n",
      "Epoch [5/5], Step [900/1875], Loss: 0.0167\n",
      "Epoch [5/5], Step [1000/1875], Loss: 0.0185\n",
      "Epoch [5/5], Step [1100/1875], Loss: 0.0188\n",
      "Epoch [5/5], Step [1200/1875], Loss: 0.0242\n",
      "Epoch [5/5], Step [1300/1875], Loss: 0.0217\n",
      "Epoch [5/5], Step [1400/1875], Loss: 0.0195\n",
      "Epoch [5/5], Step [1500/1875], Loss: 0.0276\n",
      "Epoch [5/5], Step [1600/1875], Loss: 0.0266\n",
      "Epoch [5/5], Step [1700/1875], Loss: 0.0234\n",
      "Epoch [5/5], Step [1800/1875], Loss: 0.0220\n",
      "Finished Training MNIST Model\n"
     ]
    }
   ],
   "source": [
    "# Training the MNIST Model\n",
    "\n",
    "epochs = 5\n",
    "mnist_model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(mnist_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mnist_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(mnist_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training MNIST Model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51ef8747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the CelebA test images: 89.75%\n",
      "Accuracy of the model on the MNIST test images: 98.84%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, dataset_name, multi_label=False, threshold=0.5):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            if multi_label:\n",
    "                # Apply threshold to outputs to get binary predictions for multi-label classification\n",
    "                predicted = (outputs > threshold).float()\n",
    "                # Compare predictions with actual labels\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.numel()  # Total number of labels (e.g., 40 per image)\n",
    "            else:\n",
    "                # Single-label classification (e.g., MNIST)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)  # Total number of images\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the {dataset_name} test images: {accuracy:.2f}%')\n",
    "\n",
    "# Evaluate CelebA model (multi-label classification)\n",
    "evaluate_model(celeba_model, celeba_test_loader, 'CelebA', multi_label=True)\n",
    "\n",
    "# Evaluate MNIST model (single-label classification)\n",
    "evaluate_model(mnist_model, mnist_test_loader, 'MNIST', multi_label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
